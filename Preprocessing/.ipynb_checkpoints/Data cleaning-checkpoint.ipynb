{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor \n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "\n",
    "# function that removes all redundant entries\n",
    "def remove_redundant(areas):\n",
    "    for item in areas:\n",
    "        first = item[0]\n",
    "        second = item[1]\n",
    "\n",
    "        # find the inverse of this    \n",
    "        for item in areas:\n",
    "            first_inverse = item[0]\n",
    "            second_inverse = item[1]\n",
    "\n",
    "            if first == second_inverse and second == first_inverse:\n",
    "                areas.remove(item)\n",
    "                \n",
    "    return areas\n",
    "\n",
    "# average all high correlated brain areas that are the same but on an other hemisphere\n",
    "# store other high correlated areas in a list and return them together with the new brain data\n",
    "def average_rh_lh(areas, brain_vol):\n",
    "    \n",
    "    # make a list of al areas that do not only differ in hemisphere \n",
    "    areas_diff = []\n",
    "\n",
    "    for item in areas:\n",
    "        first = item[0]\n",
    "        second = item[1]\n",
    "\n",
    "        # check if only hemisphere is the difference, if so -> average those columns\n",
    "        if first[3:] == second[3:]:\n",
    "            brain_vol.loc[:, (first[3:])] = brain_vol.apply(lambda row: (row[first] + row[second])/2, axis = 1)\n",
    "#             print(\"Brain pair combined to one:\", brain_vol[[first, second, first[3:]]])\n",
    "            brain_vol = brain_vol.drop(columns = [first, second])\n",
    "            \n",
    "        elif first[6:] == second[5:] or first[5:] == second[6:]:\n",
    "            brain_vol.loc[:, (first[5:])] = brain_vol.apply(lambda row: (row[first] + row[second])/2, axis = 1)\n",
    "#             print(\"Brain pair combined to one:\", brain_vol[[first, second, first[5:]]])\n",
    "            brain_vol = brain_vol.drop(columns = [first, second])\n",
    "            \n",
    "        elif first[6:] == second[6:]:\n",
    "            brain_vol.loc[:, 'wm_'+ (first[5:])] = brain_vol.apply(lambda row: (row[first] + row[second])/2, axis = 1)\n",
    "#             print(\"Brain pair combined to one:\", brain_vol[[first, second, 'wm_'+ (first[5:])]])\n",
    "            brain_vol = brain_vol.drop(columns = [first, second])\n",
    "        else:\n",
    "            areas_diff.append(item)\n",
    "            \n",
    "    return areas_diff, brain_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data BEAST\n",
    "data = pd.read_excel(\"BEAST_s_round.xlsx\")\n",
    "\n",
    "# create table with subid's and playerNr\n",
    "subid = data[['playerNr', 'subjectID']]\n",
    "subid = subid.dropna(subset = ['subjectID'])\n",
    "\n",
    "# drop trials without an s_round\n",
    "data = data.dropna(subset = ['s_round'])\n",
    "\n",
    "# remove data with s_round below 0 or above 1\n",
    "data = data[data.s_round >= 0]\n",
    "data = data[data.s_round <= 1]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate difference between number of animals and first estimate\n",
    "data.loc[:, ('error_estimate')] = data.apply(lambda row: row.nAnimals - row.estimate, axis = 1)\n",
    "data.loc[:, ('abs_error_estimate')] = data.apply(lambda row: abs(row.nAnimals - row.estimate), axis = 1)\n",
    "data.loc[:, ('abs_error_estimate_2')] = data.apply(lambda row: abs(row.nAnimals - row.estimate_revised), axis = 1)\n",
    "\n",
    "data = data.dropna(subset = ['abs_error_estimate', 'abs_error_estimate_2'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set threshold for the maximum reaction time and filter\n",
    "threshold = 20\n",
    "thres_20 = data[data.time_estimate < threshold]\n",
    "\n",
    "# count number of trials of participants \n",
    "counter_data = thres_20.groupby(['playerNr']).count()\n",
    "\n",
    "# set minimum number of trials trial\n",
    "min_trials = 3\n",
    "\n",
    "# store playerNr that have a minimum number of trials\n",
    "included_playerNr = counter_data[counter_data.period >= min_trials]\n",
    "included_playerNr = included_playerNr.reset_index()[['playerNr']]\n",
    "included_playerNr\n",
    "\n",
    "# see what subject IDs match those playerNr\n",
    "included_subids = included_playerNr.merge(subid).dropna()\n",
    "included_subids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate S total\n",
    "data_S = data.groupby(['playerNr']).mean()\n",
    "data_S = data_S.reset_index()\n",
    "\n",
    "# only select including subjects\n",
    "data_S = included_subids[['playerNr', 'subjectID']].merge(data_S)\n",
    "data_S = data_S[['subjectID', 's_round', 'abs_error_estimate', 'abs_error_estimate_2']].rename(columns={\"s_round\" : \"S\"})\n",
    "data_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save excel file\n",
    "#data_S.to_excel(\"BEAST_clean_abs_error_estimate_2.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ADD BRAIN AND DEMOGRAPHIC DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data rh\n",
    "data = pd.read_excel('norm_SupraTentorialVolNotVent_DK_WM_vol.xlsx').drop(columns = 'Unnamed: 0')\n",
    "\n",
    "# drop columns that are not brain areas or that we do not need\n",
    "data = data.drop(columns = ['SupraTentorialVol', 'SupraTentorialVolNotVent'])\n",
    "data = data.drop(columns = [\"wm-lh-bankssts\",\"wm-lh-caudalanteriorcingulate\",\"wm-lh-caudalmiddlefrontal\",\"wm-lh-cuneus\",\"wm-lh-entorhinal\",\"wm-lh-fusiform\",\"wm-lh-inferiorparietal\",\"wm-lh-inferiortemporal\",\"wm-lh-isthmuscingulate\",\"wm-lh-lateraloccipital\",\"wm-lh-lateralorbitofrontal\",\"wm-lh-lingual\",\"wm-lh-medialorbitofrontal\",\"wm-lh-middletemporal\",\"wm-lh-parahippocampal\",\"wm-lh-paracentral\",\"wm-lh-parsopercularis\",\"wm-lh-parsorbitalis\",\"wm-lh-parstriangularis\",\"wm-lh-pericalcarine\",\"wm-lh-postcentral\",\"wm-lh-posteriorcingulate\",\"wm-lh-precentral\",\"wm-lh-precuneus\",\"wm-lh-rostralanteriorcingulate\",\"wm-lh-rostralmiddlefrontal\",\"wm-lh-superiorfrontal\",\"wm-lh-superiorparietal\",\"wm-lh-superiortemporal\",\"wm-lh-supramarginal\",\"wm-lh-frontalpole\",\"wm-lh-temporalpole\",\"wm-lh-transversetemporal\",\"wm-lh-insula\",\"wm-rh-bankssts\",\"wm-rh-caudalanteriorcingulate\",\"wm-rh-caudalmiddlefrontal\",\"wm-rh-cuneus\",\"wm-rh-entorhinal\",\"wm-rh-fusiform\",\"wm-rh-inferiorparietal\",\"wm-rh-inferiortemporal\",\"wm-rh-isthmuscingulate\",\"wm-rh-lateraloccipital\",\"wm-rh-lateralorbitofrontal\",\"wm-rh-lingual\",\"wm-rh-medialorbitofrontal\",\"wm-rh-middletemporal\",\"wm-rh-parahippocampal\",\"wm-rh-paracentral\",\"wm-rh-parsopercularis\",\"wm-rh-parsorbitalis\",\"wm-rh-parstriangularis\",\"wm-rh-pericalcarine\",\"wm-rh-postcentral\",\"wm-rh-posteriorcingulate\",\"wm-rh-precentral\",\"wm-rh-precuneus\",\"wm-rh-rostralanteriorcingulate\",\"wm-rh-rostralmiddlefrontal\",\"wm-rh-superiorfrontal\",\"wm-rh-superiorparietal\",\"wm-rh-superiortemporal\",\"wm-rh-supramarginal\",\"wm-rh-frontalpole\",\"wm-rh-temporalpole\",\"wm-rh-transversetemporal\",\"wm-rh-insula\"])\n",
    "print(len(data.keys()))\n",
    "print(data.keys())\n",
    "\n",
    "# load demographic data\n",
    "demo = pd.read_excel('demographics.xlsx')\n",
    "demo = demo[['ID', 'gender', 'study', 'age_mri']]\n",
    "\n",
    "# merge with demo\n",
    "data = data.merge(demo)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for collinearity\n",
    "corr = data.corr()\n",
    "\n",
    "# check correlation higher than .7\n",
    "abs_cor = corr.abs()[corr<1] # remove correlation with same feature (as corr=1)\n",
    "abs_cor_unstack = abs_cor.unstack()\n",
    "sorted_cor = abs_cor_unstack.sort_values(kind = 'quicksort') # sort on how high correlation is\n",
    "\n",
    "# select correlation higher than .7 and make list of those areas\n",
    "high_cor = sorted_cor[sorted_cor > .7]\n",
    "areas = high_cor.keys()\n",
    "areas = areas.tolist()\n",
    "\n",
    "# remove redundant areas (as every correlation is now in there twice)\n",
    "areas = remove_redundant(areas)\n",
    "print(\"The amount of highly correlated brain area pairs:\", len(areas))\n",
    "areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average highly correlated brain regions (CC mid anterior and cc central are not averaged)\n",
    "averaged = average_rh_lh(areas, data)\n",
    "av_data = averaged[1]\n",
    "av_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "av_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load beast data and combine with brain data\n",
    "BEAST = pd.read_excel('BEAST_clean_abs_error_estimate_2.xlsx')\n",
    "BEAST = BEAST.rename(columns= {\"subjectID\" : \"ID\"})\n",
    "BEAST = BEAST[['ID', 'S', 'abs_error_estimate' , 'abs_error_estimate_2']]\n",
    "merge_data = av_data.merge(BEAST)\n",
    "merge_data\n",
    "\n",
    "# remove age outliers\n",
    "upperbound = merge_data.age_mri.mean() + 2 * merge_data.age_mri.std()\n",
    "print(\"removing age from:\", upperbound)\n",
    "merge_data_clean = merge_data[merge_data.age_mri < upperbound]\n",
    "print(\"number of participants left:\", len(merge_data_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_use = merge_data_clean.copy()\n",
    "\n",
    "# make a binary feature if the participant uses social information or not\n",
    "s_use['s_use'] = s_use.apply(lambda row: 1 if row.S > 0 else 0, axis = 1)\n",
    "print(\"number of pp s_use = 1 : \", len(s_use[s_use.s_use == 1 ]))\n",
    "print(\"number of pp s_use = 0 : \", len(s_use[s_use.s_use == 0 ]))\n",
    "\n",
    "# make dummie of gender\n",
    "dummy_data = pd.get_dummies(s_use, prefix=['gender'], columns=['gender'])\n",
    "\n",
    "# drop study column (not useful)\n",
    "dummy_data = dummy_data.drop(columns = 'study')\n",
    "print(\"number of males: \", len(dummy_data[dummy_data.gender_M == 1]))\n",
    "print(\"number of females: \", len(dummy_data[dummy_data.gender_F == 1]))\n",
    "print(\"number of males s_use: \", len(dummy_data[(dummy_data.gender_M == 1) & (dummy_data.s_use == 0)]))\n",
    "print(\"number of females s_use: \", len(dummy_data[(dummy_data.gender_F == 1) & (dummy_data.s_use == 0)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude participants who never used social information\n",
    "s_data = dummy_data[dummy_data.S > 0].drop(columns = ['gender_M', 'ID', 's_use', 'abs_error_estimate_2'])\n",
    "s_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data to excel\n",
    "s_data.to_excel(\"model_data_S_not_zero_absERROR_NotVent.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data to excel\n",
    "data_allS = dummy_data.drop(columns = ['gender_M', 'ID', 's_use', 'abs_error_estimate_2'])\n",
    "data_allS.to_excel(\"model_data_allS_absERROR_NotVent.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
